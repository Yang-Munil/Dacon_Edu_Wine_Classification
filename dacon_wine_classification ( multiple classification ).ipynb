{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict, StratifiedKFold, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# 일반적으로 회귀에는 기본 k-겹 교차검증을 사용하고, 분류에는 StratifiedKFold를 사용한다.\n",
    "\n",
    "# 또한, cross_val_score 함수에는 KFold의 매개변수를 제어할 수가 없으므로, \n",
    "\n",
    "# 따로 KFold 객체를 만들고 매개변수를 조정한 다음에 cross_val_score의 cv 매개변수에 넣어야 한다.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings         # warnings : 버전 충돌 및 특정 예외 처리를 위해 불러온 내장 모듈\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3,9 삭제.\n",
    "\n",
    "idx_nm = train[(train['quality'] == 3) | (train['quality'] == 9)].index\n",
    "train = train.drop(idx_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2416\n",
       "5    1788\n",
       "7     924\n",
       "4     186\n",
       "8     152\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3,9 삭제\n",
    "\n",
    "train['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더미변수 생성\n",
    "\n",
    "df1 = pd.get_dummies(train['type'], prefix = 'type_' ,drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, df1], axis=1 )\n",
    "train.drop('type', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5466, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5466,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train['quality']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5466, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.drop('quality', axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN\n",
    "- ADASYN(Adaptive Synthetic Sampling) 방법은 소수 클래스 데이터와 그 데이터에서 가장 가까운 k개의 소수 클래스 데이터 중 무작위로 선택된 데이터 사이의 직선상에 가상의 소수 클래스 데이터를 만드는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = ADASYN(random_state=0).fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 세트 Shape:(9395, 13), 테스트 세트 Shape:(1094, 13)\n",
      " 학습 세트 레이블 값 분포 비율\n",
      "6    0.205641\n",
      "8    0.204364\n",
      "4    0.203406\n",
      "5    0.199255\n",
      "7    0.187334\n",
      "Name: quality, dtype: float64\n",
      "\n",
      " 테스트 세트 레이블 값 분포 비율\n",
      "6    0.442413\n",
      "5    0.327239\n",
      "7    0.169104\n",
      "4    0.033821\n",
      "8    0.027422\n",
      "Name: quality, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 레이블 값 분포를 보기위한 코드.\n",
    "\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape , X_test.shape))\n",
    "\n",
    "print(' 학습 세트 레이블 값 분포 비율')\n",
    "print(y_train.value_counts()/train_cnt)\n",
    "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
    "print(y_test.value_counts()/test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rbf logistic is  0.2477148080438757\n",
      "The cross validated score for logistic is: 0.3814609839461881\n"
     ]
    }
   ],
   "source": [
    "# logistic\n",
    "\n",
    "log_cl =  LogisticRegression(n_jobs = -1)\n",
    "log_cl.fit(X_train, y_train)\n",
    "log_cl_pred = log_cl.predict(X_test)\n",
    "print('Accuracy for rbf logistic is ', metrics.accuracy_score(log_cl_pred, y_test))\n",
    "\n",
    "log_cl_cv = cross_val_score(log_cl, X_train, y_train, cv = 10, scoring = 'accuracy', n_jobs=-1)\n",
    "print('The cross validated score for logistic is:', log_cl_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Random Forests is 0.6672760511882998\n",
      "The cross validated score for Random forest is: 0.8144777150884825\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "\n",
    "rf_cl = RandomForestClassifier(n_estimators = 1000, n_jobs = -1)\n",
    "rf_cl.fit(X_train, y_train)\n",
    "rf_cl_pred = rf_cl.predict(X_test)\n",
    "print('The accuracy of the Random Forests is', metrics.accuracy_score(rf_cl_pred, y_test))\n",
    "\n",
    "rf_cl_cv = cross_val_score(rf_cl, X_train, y_train, cv = 10, scoring = 'accuracy', n_jobs=-1)\n",
    "print('The cross validated score for Random forest is:', rf_cl_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for bagged KNN is: 0.4287020109689214\n",
      "The cross validated score for Bagged KNN is: 0.6687429537138533\n"
     ]
    }
   ],
   "source": [
    "# Bagged KNN\n",
    "\n",
    "ba_knn_model=BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors=4), n_estimators=200, n_jobs=-1)\n",
    "ba_knn_model.fit(X_train, y_train)\n",
    "ba_knn_model_pred = ba_knn_model.predict(X_test)\n",
    "print('The accuracy for bagged KNN is:', metrics.accuracy_score(ba_knn_model_pred, y_test))\n",
    "\n",
    "ba_knn_model_cv = cross_val_score(ba_knn_model, X_train, y_train, cv=10, scoring = 'accuracy', n_jobs=-1)\n",
    "print('The cross validated score for Bagged KNN is:', ba_knn_model_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for bagged Decision Tree is: 0.6279707495429616\n",
      "The cross validated score for bagged Decision Tree is: 0.7915628746391744\n"
     ]
    }
   ],
   "source": [
    "# Bagged DecisionTree\n",
    "\n",
    "ba_dt_model = BaggingClassifier(base_estimator = DecisionTreeClassifier(), random_state=0, n_estimators=100, n_jobs=-1)\n",
    "ba_dt_model.fit(X_train, y_train)\n",
    "ba_dt_model_pred = ba_dt_model.predict(X_test)\n",
    "print('The accuracy for bagged Decision Tree is:', metrics.accuracy_score(ba_dt_model_pred, y_test))\n",
    "\n",
    "ba_dt_model_cv = cross_val_score(ba_dt_model, X_train, y_train, cv = 10, scoring = 'accuracy', n_jobs=-1)\n",
    "print('The cross validated score for bagged Decision Tree is:', ba_dt_model_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for bagged Decision Tree is: 0.3702010968921389\n",
      "The cross validated score for AdaBoost is: 0.46274236433736443\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost (Adaptive boosting)\n",
    "\n",
    "ada_cl = AdaBoostClassifier(n_estimators = 200, learning_rate = 0.1)\n",
    "ada_cl.fit(X_train, y_train)\n",
    "ada_cl_pred = ada_cl.predict(X_test)\n",
    "print('The accuracy for bagged Decision Tree is:', metrics.accuracy_score(ada_cl_pred, y_test))\n",
    "\n",
    "ada_cl_cv = cross_val_score(ada_cl, X_train, y_train, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "print('The cross validated score for AdaBoost is:', ada_cl_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Gradient Boost is: 0.5438756855575868\n",
      "The cross validated score for Gradient Boosting is: 0.6861878280360145\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boost\n",
    "\n",
    "gr_grad = GradientBoostingClassifier(n_estimators = 200, learning_rate = 0.1)\n",
    "gr_grad.fit(X_train, y_train)\n",
    "gr_grad_pred = gr_grad.predict(X_test)\n",
    "print('The accuracy for Gradient Boost is:', metrics.accuracy_score(gr_grad_pred, y_test))\n",
    "\n",
    "gr_grad_cv = cross_val_score(gr_grad, X_train, y_train, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "print('The cross validated score for Gradient Boosting is:', gr_grad_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the XGBoost is 0.6288848263254113\n",
      "The cross validated score for XGBoost is: 0.7759253099249662\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators = 200, learning_rate = 0.1)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "xgb_model_pred = xgb_model.predict(X_test)\n",
    "print('The accuracy of the XGBoost is', metrics.accuracy_score(xgb_model_pred, y_test))\n",
    "\n",
    "xgb_model_cv = cross_val_score(xgb_model, X_train, y_train, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "print('The cross validated score for XGBoost is:', xgb_model_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Light GBM is 0.6535648994515539\n",
      "The cross validated score for Light GBM is: 0.8145933881675844\n"
     ]
    }
   ],
   "source": [
    "# Light GBM\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=400, n_jobs = -1)\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "lgbm_clf_pred = lgbm_clf.predict(X_test)\n",
    "print('The accuracy of the Light GBM is', metrics.accuracy_score(lgbm_clf_pred, y_test))\n",
    "\n",
    "lgbm_clf_cv = cross_val_score(lgbm_clf, X_train, y_train, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "print('The cross validated score for Light GBM is:', lgbm_clf_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>CV_Mean</th>\n",
       "      <th>CV_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Light GBM</th>\n",
       "      <td>0.649909</td>\n",
       "      <td>0.813661</td>\n",
       "      <td>0.032592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest</th>\n",
       "      <td>0.659963</td>\n",
       "      <td>0.812596</td>\n",
       "      <td>0.018319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagged DecisionTree</th>\n",
       "      <td>0.627971</td>\n",
       "      <td>0.791563</td>\n",
       "      <td>0.016156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.628885</td>\n",
       "      <td>0.775925</td>\n",
       "      <td>0.022995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost</th>\n",
       "      <td>0.543876</td>\n",
       "      <td>0.686188</td>\n",
       "      <td>0.024705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagged KNN</th>\n",
       "      <td>0.428702</td>\n",
       "      <td>0.668743</td>\n",
       "      <td>0.017401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adaptive Boost</th>\n",
       "      <td>0.370201</td>\n",
       "      <td>0.462742</td>\n",
       "      <td>0.016648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Classifier</th>\n",
       "      <td>0.247715</td>\n",
       "      <td>0.381461</td>\n",
       "      <td>0.032185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy   CV_Mean    CV_std\n",
       "Model                                            \n",
       "Light GBM            0.649909  0.813661  0.032592\n",
       "Random forest        0.659963  0.812596  0.018319\n",
       "Bagged DecisionTree  0.627971  0.791563  0.016156\n",
       "XGBoost              0.628885  0.775925  0.022995\n",
       "Gradient Boost       0.543876  0.686188  0.024705\n",
       "Bagged KNN           0.428702  0.668743  0.017401\n",
       "Adaptive Boost       0.370201  0.462742  0.016648\n",
       "Logistic Classifier  0.247715  0.381461  0.032185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Classifier',\n",
    "              'Random forest', 'Bagged KNN', 'Bagged DecisionTree', 'Adaptive Boost', \n",
    "              'Gradient Boost', 'XGBoost', 'Light GBM'],\n",
    "    'Accuracy': [metrics.accuracy_score(log_cl_pred, y_test),  \n",
    "                 metrics.accuracy_score(rf_cl_pred, y_test), metrics.accuracy_score(ba_knn_model_pred, y_test),\n",
    "                 metrics.accuracy_score(ba_dt_model_pred, y_test), metrics.accuracy_score(ada_cl_pred, y_test),\n",
    "                 metrics.accuracy_score(gr_grad_pred, y_test), metrics.accuracy_score(xgb_model_pred, y_test),\n",
    "                 metrics.accuracy_score(lgbm_clf_pred, y_test)],\n",
    "    'CV_Mean': [log_cl_cv.mean(), rf_cl_cv.mean(), ba_knn_model_cv.mean(),\n",
    "                ba_dt_model_cv.mean(), ada_cl_cv.mean(), gr_grad_cv.mean(), \n",
    "                xgb_model_cv.mean(), lgbm_clf_cv.mean()],\n",
    "    'CV_std':[log_cl_cv.std(), rf_cl_cv.std(), ba_knn_model_cv.std(),\n",
    "                ba_dt_model_cv.std(), ada_cl_cv.std(), gr_grad_cv.std(), \n",
    "                xgb_model_cv.std(), lgbm_clf_cv.std()]\n",
    "})\n",
    "\n",
    "result = results.sort_values(by='CV_Mean', ascending=False)\n",
    "result = result.set_index('Model')\n",
    "display(result.head(13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 하이퍼파라미터 수정 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': [6, 8, 10, 12],\n",
       "                         'min_samples_leaf': [8, 12, 18],\n",
       "                         'min_samples_split': [8, 16, 20],\n",
       "                         'n_estimators': [10, 100]},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Random forest 하이퍼파라미터 수정\n",
    "\n",
    "hyper = { 'n_estimators' : [10, 100],\n",
    "           'max_depth' : [6, 8, 10, 12],\n",
    "           'min_samples_leaf' : [8, 12, 18],\n",
    "           'min_samples_split' : [8, 16, 20]\n",
    "            }\n",
    "\n",
    "gd_rf = GridSearchCV(estimator = RandomForestClassifier(), param_grid = hyper, verbose = True, n_jobs=-1)\n",
    "gd_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=12, min_samples_leaf=8, min_samples_split=8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gd_best = RandomForestClassifier(max_depth=12, min_samples_leaf=8, min_samples_split=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=12, min_samples_leaf=8, min_samples_split=8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gd_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# {'bootstrap': [True, False],\n",
    "#  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "#  'max_features': ['auto', 'sqrt'],\n",
    "#  'min_samples_leaf': [1, 2, 4],\n",
    "#  'min_samples_split': [2, 5, 10],\n",
    "#  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 15.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7913807108062185\n",
      "RandomForestClassifier(bootstrap=False, max_depth=90, max_features='sqrt',\n",
      "                       n_estimators=1200)\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "print(rf_random.best_score_)\n",
    "print(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random_best = RandomForestClassifier(bootstrap=False, max_depth=90, max_features='sqrt',\n",
    "                       n_estimators=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=90, max_features='sqrt',\n",
       "                       n_estimators=1200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 제출용 테스트데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더미변수 생성\n",
    "\n",
    "df1 = pd.get_dummies(test['type'], prefix = 'type_' ,drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test, df1], axis=1 )\n",
    "test.drop('type', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 제출용파일생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. 랜덤포레스트 최적(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cl_pred = rf_cl.predict(test)\n",
    "rf_cl_pred = pd.DataFrame(rf_cl_pred)\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['quality'] = rf_cl_pred[0]\n",
    "submit.to_csv('./sub_rf_cl.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. 랜덤포레스트 튜닝모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gd_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=12, min_samples_leaf=8, min_samples_split=16,\n",
       "                       n_jobs=-1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gd_best_pred = rf_gd_best.predict(test)\n",
    "rf_gd_best_pred = pd.DataFrame(rf_gd_best_pred)\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['quality'] = rf_gd_best_pred[0]\n",
    "submit.to_csv('./sub_rf_gd_best.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random_best_pred = rf_random_best.predict(test)\n",
    "rf_random_best_pred = pd.DataFrame(rf_random_best_pred)\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['quality'] = rf_random_best_pred[0]\n",
    "submit.to_csv('./sub_rf_random_best.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. Light GBM 최적(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(n_estimators=400)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf_pred = lgbm_clf.predict(test)\n",
    "lgbm_clf_pred = pd.DataFrame(lgbm_clf_pred)\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['quality'] = lgbm_clf_pred[0]\n",
    "submit.to_csv('./sub_lgbm_clf.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4. Light GBM 최적(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_best = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "               importance_type='split', lambda_l1=1, lambda_l2=1,\n",
    "               learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
    "               min_child_weight=0.001, min_data_in_leaf=30, min_split_gain=0.0,\n",
    "               n_estimators=200, n_jobs=-1, num_leaves=127, objective=None,\n",
    "               random_state=None, reg_alpha=0.1, reg_lambda=0.0, silent=True,\n",
    "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(lambda_l1=1, lambda_l2=1, min_data_in_leaf=30, n_estimators=200,\n",
       "               num_leaves=127, reg_alpha=0.1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_best_pred = lg_best.predict(test)\n",
    "lg_best_pred = pd.DataFrame(lg_best_pred)\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['quality'] = lg_best_pred[0]\n",
    "submit.to_csv('./sub_lg_best.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
